{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance and RFE for college enrollment\n",
    "#### (With the trained logistic regression and random forests models trained in Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in data\n",
    "X=np.loadtxt('dat1_collen.csv', delimiter=',')\n",
    "collen=np.loadtxt('collen.csv', delimiter=',')\n",
    "weights=np.loadtxt('weights4_collen.csv', delimiter=',')\n",
    "\n",
    "#setting us same train-test data as in step 2\n",
    "skf = StratifiedKFold(n_splits=5, random_state = 666, shuffle= True)\n",
    "skf.get_n_splits(X, collen)\n",
    "train_indices=[]\n",
    "test_indices=[]\n",
    "for train_index, test_index in skf.split(X, collen):\n",
    "    train_indices.append(train_index)\n",
    "    test_indices.append(test_index)\n",
    "\n",
    "train1 = train_indices[0]\n",
    "test1 = test_indices[0]\n",
    "train2 = train_indices[1]\n",
    "test2 = test_indices[1]\n",
    "train3 = train_indices[2]\n",
    "test3 = test_indices[2]\n",
    "train4 = train_indices[3]\n",
    "test4 = test_indices[3]\n",
    "train5 = train_indices[4]\n",
    "test5 = test_indices[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance -- logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV1\n",
    "lr1 = LogisticRegression(random_state = 666, C = 10)\n",
    "lr1.fit(X[train1], collen[train1], sample_weight = weights[train1])\n",
    "coefs1 = lr1.coef_\n",
    "\n",
    "#CV2\n",
    "lr2 = LogisticRegression(random_state = 666, C = 1000)\n",
    "lr2.fit(X[train2], collen[train2], sample_weight = weights[train2])\n",
    "coefs2 = lr2.coef_\n",
    "\n",
    "#CV3\n",
    "lr3 = LogisticRegression(random_state = 666, C = 1)\n",
    "lr3.fit(X[train3], collen[train3], sample_weight = weights[train3])\n",
    "coefs3 = lr3.coef_\n",
    "\n",
    "#CV4\n",
    "lr4 = LogisticRegression(random_state = 666, C = .002)\n",
    "lr4.fit(X[train4], collen[train4], sample_weight = weights[train4])\n",
    "coefs4 = lr4.coef_\n",
    "\n",
    "#CV5\n",
    "lr5 = LogisticRegression(random_state = 666, C = .01)\n",
    "lr5.fit(X[train5], collen[train5], sample_weight = weights[train5])\n",
    "coefs5 = lr5.coef_\n",
    "\n",
    "\n",
    "#calculate the average\n",
    "coefs_collen = np.concatenate (([coefs1_std], [coefs2_std], [coefs3_std], [coefs4_std], [coefs5_std]), axis=0)\n",
    "print(coefs_collen)\n",
    "coefs_collen_avg = np.mean(coefs_collen, axis = 0)\n",
    "coefs_collen_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance --random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV1\n",
    "rf1 = RandomForestClassifier(random_state = 666, n_estimators=400, max_depth = 7) \n",
    "rf1.fit(X[train1], collen[train1], sample_weight = weights[train1])\n",
    "imps1 = rf1.feature_importances_\n",
    "\n",
    "#CV2\n",
    "rf2 = RandomForestClassifier(random_state = 666, n_estimators=800, max_depth = 10) \n",
    "rf2.fit(X[train2], collen[train2], sample_weight = weights[train2])\n",
    "imps2 = rf2.feature_importances_\n",
    "\n",
    "#CV3\n",
    "rf3 = RandomForestClassifier(random_state = 666, n_estimators=900, max_depth = 10) \n",
    "rf3.fit(X[train3], collen[train3], sample_weight = weights[train3])\n",
    "imps3 = rf3.feature_importances_\n",
    "\n",
    "#CV4\n",
    "rf4 = RandomForestClassifier(random_state = 666, n_estimators=400, max_depth = 11) \n",
    "rf4.fit(X[train4], collen[train4], sample_weight = weights[train4])\n",
    "imps4 = rf4.feature_importances_\n",
    "\n",
    "#CV5\n",
    "rf5 = RandomForestClassifier(random_state = 666, n_estimators=700, max_depth = 20) \n",
    "rf5.fit(X[train5], collen[train5], sample_weight = weights[train5])\n",
    "imps5 = rf5.feature_importances_\n",
    "\n",
    "#Calculate the average\n",
    "imps_collen = np.concatenate (([imps1], [imps2], [imps3], [imps4], [imps5]), axis=0)\n",
    "imps_collen_avg = np.mean(imps_collen, axis = 0)\n",
    "imps_collen_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive feature elimination -- logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Mother', 'Father', 'TwoBioParent', 'HHsize', 'SibNum', 'BirthOrder', 'momeduc', 'dadeduc', \n",
    "                 'momjob', 'dadjob', 'faminc', 'PAassistance', 'welfare', 'PAecohard', 'minvolve', 'dinvolve',\n",
    "                 'PAPTA', 'mexp', 'dexp', 'mactiv', 'dactiv', 'control', 'mspv', 'dspv', 'mrel', 'drel', \n",
    "                 'famsup', 'dinner', 'PAclosure', 'mnativity', 'dnativity', 'PAage', 'PAhealth', 'PAsmoke', \n",
    "                 'malcohol', 'dalcohol', 'mobese', 'dobese', 'mdisable', 'ddisable', 'PArelig', 'HHsmoke', \n",
    "                 'HHdrug', 'fammed', 'EnglishHome', 'biosex', 'YAge', 'Latino', 'AA', 'Native', 'Asian', 'other_race',\n",
    "                 'nativity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_Regs = [10, 1000, 1, 0.002, 0.01]\n",
    "X_rfe = X\n",
    "accuracies = []\n",
    "feature_eliminate = []\n",
    "while len(feature_names)>0:  #stop till all features are eliminated\n",
    "    accuracy = []\n",
    "    coefs_all=[]\n",
    "    for i in range(5):\n",
    "        train = train_indices[i]\n",
    "        test = test_indices[i]\n",
    "        lr = LogisticRegression(random_state = 666, C = optimal_Regs[i])\n",
    "        lr.fit(X_rfe[train], collen[train], sample_weight = weights[train])\n",
    "        acc = lr.score(X_rfe[test], collen[test], sample_weight=weights[test])\n",
    "        accuracy.append(acc)\n",
    "        coefs = lr.coef_\n",
    "        coefs_std = np.std(X_rfe, 0)*coefs\n",
    "        coefs_all.append(coefs_std)    #outer loop 5-fold cross-validation for accuracy calculation at each round\n",
    "    accuracy_avg = np.mean(accuracy)\n",
    "    print(accuracy_avg)\n",
    "    accuracies.append(accuracy_avg)\n",
    "    coefs_all_arr = np.concatenate ((coefs_all), axis=0)\n",
    "    coefs_all_avg = np.mean(coefs_all_arr, axis = 0)\n",
    "    print(coefs_all_avg)\n",
    "    coefs_all_avg_abs = np.absolute(coefs_all_avg)\n",
    "    least_imp_ind = np.argmin(coefs_all_avg_abs)\n",
    "    print(feature_names[least_imp_ind], least_imp_ind)\n",
    "    feature_eliminate.append(feature_names[least_imp_ind])\n",
    "    feature_names.pop(least_imp_ind)\n",
    "    X_rfe = np.delete(X_rfe, least_imp_ind, 1)  #deleting the least important feature\n",
    "    print(X_rfe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_eliminate)\n",
    "#eliminated feature order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the RFE curve\n",
    "eli = range(53)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.plot(eli, accuracies)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features eliminated')\n",
    "plt.tight_layout()\n",
    "plt.savefig('collen_lr_rfe', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive feature elimination -- random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rfe = X\n",
    "accuracies = []\n",
    "feature_eliminate = []\n",
    "ntree_rfe = [400, 800, 900, 400, 700]\n",
    "depths_rfe = [7, 10, 10, 11, 20]\n",
    "while len(feature_names)>0:\n",
    "    accuracy = []\n",
    "    imps_all=[]\n",
    "    for i in range(5):\n",
    "        train = train_indices[i]\n",
    "        test = test_indices[i]\n",
    "        rf = RandomForestClassifier(random_state = 666, n_estimators=ntree_rfe[i], max_depth = depths_rfe[i]) \n",
    "        rf.fit(X_rfe[train], collen[train], sample_weight = weights[train])\n",
    "        acc = rf.score(X_rfe[test], collen[test], sample_weight=weights[test])\n",
    "        accuracy.append(acc)\n",
    "        imps = rf.feature_importances_\n",
    "        #print('imps',imps)\n",
    "        imps_all.append(imps)\n",
    "        #print('imps_all',imps_all)\n",
    "        print(i, acc)\n",
    "    accuracy_avg = np.mean(accuracy)\n",
    "    print('accuracy', accuracy_avg)\n",
    "    accuracies.append(accuracy_avg)\n",
    "    imps_all_avg = np.mean(imps_all, axis = 0)\n",
    "    print('imps_all_avg', imps_all_avg)\n",
    "    least_imp_ind = np.argmin(imps_all_avg)\n",
    "    print(feature_names[least_imp_ind], least_imp_ind)\n",
    "    feature_eliminate.append(feature_names[least_imp_ind])\n",
    "    feature_names.pop(least_imp_ind)\n",
    "    X_rfe = np.delete(X_rfe, least_imp_ind, 1)\n",
    "    print(X_rfe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_eliminate)\n",
    "#eliminated feature order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "eli = range(53)\n",
    "plt.plot(eli, accuracies)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features eliminated')\n",
    "plt.tight_layout()\n",
    "plt.savefig('collen_rf_rfe', dpi=1000)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
